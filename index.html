<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tianhao Cheng (ç¨‹å¤©è±ª)</title>

  <meta name="author" content="Xinyu Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘¾</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Tianhao Cheng (ç¨‹å¤©è±ª)</name>
                  </p>
                  <p>I am a second-year master student at <a href="https://cs.fudan.edu.cn/"> the School of Computer
                      Science Fudan University</a>, advised by
                    Prof. <a href="https://faculty.fudan.edu.cn/fengrui/zh_CN/index.htm">Rui Feng</a>. I'm honored to be
                    guided by Professor <a href="https://bigaidream.github.io/">Fujie</a>.
                    Also, I'm a research intern at <a href="https://www.infly.cn/">INFTECH</a> with Researcher <a
                      href="https://commencement.github.io/">Zili Wang</a> focusing on LLM pretrain.
                  </p>
                  <p>
                    <!-- My research interests include computer vision and multi-modality. I created the <a href="https://github.com/xinyu1205/recognize-anything" style="color: red;">Recognize Anything Model (RAM) Family</a>, which is a series of open-source and powerful image recognition models. -->
                    My research interests include LLM (pretrain, post-train), system-2 LLMs. I'm a member of <a
                      href="https://github.com/infly-ai/INF-LLM">INF-34B</a> pretrain team, focusing on code part. I'm
                    currently working on a new coder model train from scratch.
                  </p>
                  <p>
                    <!-- <strong>I expect to graduate at June 2025. I am opening to both academic positions and industrial research positions. Kindly download my <a href="images/resume_xinyuhuang.pdf">Resume</a>, and do not hesitate to email me if you're interested :)</strong> -->
                    It's fun to seek the principles underlying the dark magic. AGI belongs to those who love it :-)
                  </p>
                  <p style="text-align:center">
                    <!--                 <a href="images/resume_xinyuhuang.pdf">Resume</a> &nbsp/&nbsp -->
                    <a href="mailto:thcheng23@m.fudan.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=LwvEAgoAAAAJ&hl=zh-CN">Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/crazycth">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/yi-jin-ye-xing-4-38">Zhihu</a>
                  </p>
                  <!-- </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xinyuhuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xinyuhuang.jpg" class="hoverZoomLink"></a>
            </td> -->
                <td style="padding:1.5%;width:40%;max-width:40%">
                  <a href="images/æˆ‘.png"><img style="width:80%;max-width:80" alt="profile photo" src="images/æˆ‘.png"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <heading>Research</heading>(* indicates equal contribution) -->
                  <heading>Research</heading> (* indicates equal contribution)
                </td>
              </tr>
            </tbody>
          </table>
          <style>
            img {
              border-radius: 15px;
            }
          </style>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/localization_and_recognition.jpg" alt="tag2text" width="190" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://recognize-anything.github.io/">
                    <!-- <papertitle> Recognize Anything Model (RAM) </papertitle> -->
                  </a>
                  <papertitle> <br>CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart
                  </papertitle>
                  <br>
                  Bowen Zhao,
                  <strong>Tianhao Cheng</strong>,
                  Yuejie Zhang, Ying Cheng, Rui Feng, Xiaobo Zhang
                  <br>
                  <strong><em>ACM MM2024, ORAL</em></strong>
                  <br>
                  <a href="https://openreview.net/forum?id=4sqXrISHZT">paper</a>
                  /
                  <!-- <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              / -->
                  <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              / -->
                  <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
                  <p></p>
                  <!-- <p>CT2C-QA is a benchmark </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/nature.png" alt="tag2text" width="190" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2310.15200">
                    <!-- <papertitle> Recognize Anything Plus Model (RAM++) </papertitle> -->
                  </a>
                  <papertitle> <br> RETFound-enhanced community-based fundus disease screening: real-world evidence and
                    decision curve analysis </papertitle>
                  <br>
                  Juzhao Zhang*, Senlin Lin*, <strong>Tianhao Cheng*</strong>, Yi Xu, Lina Lu, Jiangnan He, Tao Yu,
                  Yajun Peng, Yuejie Zhang, Haidong Zhou, Yingyan Ma
                  <br>
                  <em>npj Digital Medicine (NATURE RESEARCH, IF 12.4)</em>
                  <br>
                  <!-- <a href="https://recognize-anything.github.io/">project page</a> -->

                  <a href="https://www.nature.com/articles/s41746-024-01109-5">paper</a>
                  /
                  <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a> -->

                  <a
                    href="https://github.com/Akemimadokami/DL-Model-for-Community-based-Fundus-Disease-Screening">code</a>
                  <p></p>
                  <!-- <p>RAM++ is the next generation of RAM, which can <strong>recognize any category with high accuracy</strong>, including <strong>both predefined common categories and diverse open-set categories</strong>.    </p> -->
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects & Resources</heading>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/INF-LLM2.png" alt="tag2text" width="190" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://github.com/infly-ai/INF-LLM">
                    <papertitle> INF-34B: INFâ€™s Open-Source Large Language Models </papertitle>
                  </a>
                  <br>
                  <strong>Pretrain Team Member</strong>
                  <br>
                  <!-- <span style="color: red;">2.4K+ stars! </span> -->
                  <p></p>
                  <p>INF-34B has 34 billion parameters with a context window length of 32K, and is trained on about 3.5T
                    well-processed tokens from English and Chinese bilingual corpus.</p>
                </td>
              </tr>


              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src="images/YiVal.png" alt="tag2text" width="190" height="110">
                    </td>
                    <td width="75%" valign="middle">
                      <a href="https://github.com/YiVal/YiVal">
                        <papertitle> YiVal: Automatic Prompt Engineering Assistant </papertitle>
                      </a>
                      <br>
                      <strong>Project Co-Leader & Main Contributor (1w+ code merge)</strong>
                      <br>
                      <span style="color: red;">2.5K+ stars! </span>
                      <p></p>
                      <p>YiVal is a framework designed for <strong>prompt engineering</strong> especially in
                        <strong>auto-prompting</strong></p>
                      <p>Fun FactðŸŒŸ: This was my first startup experience. We ultimately failed, but I love the feeling
                        of business. </p>
                    </td>
                  </tr>
                </tbody>
              </table>


            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Experiences</heading>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/localization_and_recognition.jpg" alt="tag2text" width="190" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://recognize-anything.github.io/">
                    <!-- <papertitle> Recognize Anything Model (RAM) </papertitle> -->
                  </a>
                  <papertitle> <br>CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart
                  </papertitle>
                  <br>
                  Bowen Zhao,
                  <strong>Tianhao Cheng</strong>,
                  Yuejie Zhang, Ying Cheng, Rui Feng, Xiaobo Zhang
                  <br>
                  <strong><em>ACM MM2024, ORAL</em></strong>
                  <br>
                  <a href="https://openreview.net/forum?id=4sqXrISHZT">paper</a>
                  /
                  <!-- <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              / -->
                  <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              / -->
                  <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
                  <p></p>
                  <!-- <p>CT2C-QA is a benchmark </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/nature.png" alt="tag2text" width="190" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2310.15200">
                    <!-- <papertitle> Recognize Anything Plus Model (RAM++) </papertitle> -->
                  </a>
                  <papertitle> <br> RETFound-enhanced community-based fundus disease screening: real-world evidence and
                    decision curve analysis </papertitle>
                  <br>
                  Juzhao Zhang*, Senlin Lin*, <strong>Tianhao Cheng*</strong>, Yi Xu, Lina Lu, Jiangnan He, Tao Yu,
                  Yajun Peng, Yuejie Zhang, Haidong Zhou, Yingyan Ma
                  <br>
                  <em>npj Digital Medicine (NATURE RESEARCH, IF 12.4)</em>
                  <br>
                  <!-- <a href="https://recognize-anything.github.io/">project page</a> -->

                  <a href="https://www.nature.com/articles/s41746-024-01109-5">paper</a>
                  /
                  <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a> -->

                  <a
                    href="https://github.com/Akemimadokami/DL-Model-for-Community-based-Fundus-Disease-Screening">code</a>
                  <p></p>
                  <!-- <p>RAM++ is the next generation of RAM, which can <strong>recognize any category with high accuracy</strong>, including <strong>both predefined common categories and diverse open-set categories</strong>.    </p> -->
                </td>
              </tr>

            </tbody>
          </table>



            <!-- <a href="https://clustrmaps.com/site/1c11x"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=0MR5uesZjfPI7YeX0flw1xHWcwGVol-FCRcfFdgQvfs&cl=ffffff" /></a> -->

        </td>
      </tr>

      <h2 style="CLEAR: both;"></h2>
      <script type="text/javascript" id="clustrmaps" 
      src="//clustrmaps.com/map_v2.js?d=0MR5uesZjfPI7YeX0flw1xHWcwGVol-FCRcfFdgQvfs&cl=080808&w=300&t=tt&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080">
      </script>    

  </table>
</body>
</html>